<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2.1.14. wff_language.regex_trie_dict_scanner module &mdash; Skolem  documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Skolem  documentation" href="index.html" />
    <link rel="up" title="2. wff_language package" href="wff_language.html" />
    <link rel="next" title="2.1.15. wff_language.set_package_attribute module" href="wff_language.set_package_attribute.html" />
    <link rel="prev" title="2.1.13. wff_language.regex_trie_dict_lexer module" href="wff_language.regex_trie_dict_lexer.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="wff_language.set_package_attribute.html" title="2.1.15. wff_language.set_package_attribute module"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="wff_language.regex_trie_dict_lexer.html" title="2.1.13. wff_language.regex_trie_dict_lexer module"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Skolem  documentation</a> &raquo;</li>
          <li><a href="wff_language.html" accesskey="U">2. wff_language package</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-wff_language.regex_trie_dict_scanner">
<span id="wff-language-regex-trie-dict-scanner-module"></span><h1>2.1.14. wff_language.regex_trie_dict_scanner module<a class="headerlink" href="#module-wff_language.regex_trie_dict_scanner" title="Permalink to this headline">¶</a></h1>
<div class="section" id="regextriedictscanner">
<h2>2.1.14.1. RegexTrieDictScanner<a class="headerlink" href="#regextriedictscanner" title="Permalink to this headline">¶</a></h2>
<p>The RegexTrieDictScanner uses a RegexTrieDict for tokenizing a sequence of
elements.  This class essentially does the same thing that re.findall would do
in a traditional scanner design, except that it is easy to dynamically update
it (though it is less efficient at the static matching).</p>
<p>The elements (usually characters) are inserted one by one and tokens
are returned when they are recognized.  The key-sequences inserted into the
RegexTrieDict are by definition the tokens.  An arbitrary sequence can then be
tokenized by inserting it element by element into the TrieDict, using a special
method.  The shortest or longest matches can be found.  (Note that for
generality we refer to &#8220;tokens&#8221; and the &#8220;elements&#8221; that make up both the tokens
and the sequences to be tokenized.  In lexical analysis applications the tokens
are strings and the elements are characters.)</p>
<blockquote>
<div><p>tok = RegexTreeDictTokenizer(td)</p>
<dl class="docutils">
<dt>for i in &#8220;eggbert&#8221;:</dt>
<dd>tok.insertSeqElem(i)
tok.printTokenDeque()</dd>
<dt>for i in &#8220;eggber&#8221;:</dt>
<dd>tok.insertSeqElem(i)
tok.printTokenDeque()</dd>
</dl>
<p>tok.insertSeqElem(&#8220;x&#8221;)</p>
</div></blockquote>
<p>The results of the tokenization are automatically place in a deque which is
stored with the RegexTreeDictTokenizer instance.  Users can manipulate this
deque in any way they want; it is only used for reporting tokens as they are
unambiguously detected (i.e., they are inserted when matched).  Note that it
may be necessary to call tok.assertEndOfSequence() in order for the tokenizer
to deal with situations that are currently ambiguous (as far as finding the
longest match).</p>
<blockquote>
<div>tok.assertEndOfSequence()
tok.printTokenDeque()
tok.clearDeque()</div></blockquote>
<p>Key strings can be matched as tokens from a sequential character stream,
choosing either the longest or the shortest (first) match.  Finding the
shortest matches (assuming no regexes) is linear in the overall query string
length.  The time when finding the longest matches is still efficient in the
usual cases but is not linear in the query-lengths because recursion is used to
effectively back up when it becomes known that a recognized pattern is the
longest (in that part of the sequence).  But it must wait for a mismatch or the
end of the query string to know that a saved possible match was the longest.</p>
<dl class="docutils">
<dt>Worst case, suppose we have these three keys:</dt>
<dd>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
a
b</dd>
<dt>Now, suppose the input query-stream of characters is:</dt>
<dd>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab</dd>
</dl>
<p>We save the first a as a possible match, and then we have to go all the way to
the final b to determine that the longer string is not a match (and the single
character is the longest match).  Then we go back and start the same thing over
from the second a, and so forth, getting a time which depends on the length of
the longest string stored and the prefix properties of the stored strings.
When the stored strings are relatively short relative to the query length and
are distributed in &#8220;the usual ways&#8221; this should not make much difference in
practice.</p>
<p>The tree can easily handle whitespace characters if whitespace characters are
never valid stored strings or substrings of stored strings.  We just get an
unrecognized string result on those characters (depending on how error handling
is configured, with fastRecover of not).  Alternately, and better, we can
insert one of each whitespace character into the tree and then just test and
ignore those matches.  Or we could just do a split on whitespace before feeding
data to the tree; that is probably best in most situations.</p>
<p>For queries of fixed keys which are either in the structure or not there is no
real advantage to using the tree algorithm (it will be slower since it uses a
standard dicts at each node for storing child nodes).  What the tree algorithm
can do well is recognize stored items (tokens) out of a continuous, sequential
stream of characters (either finding the first match or the longest), while
also allowing fast inserts and deletes of keys/tokens.</p>
<p>Using a standard hashed dict (or REs) for the same thing we would build up the
query string character by character and query the hash dict on the query string
each time a character is appended to it.  That is, generate the prefixes of the
input character string.  Then it would save possible matches, etc., and when a
match is recognized as the longest it would remove that prefix and restart,
just like the tree version below.  To find the longest match, however, we need
to know when a mismatch occurs or else go all the way to the end of the input
each time.  This would entail saving all the prefixes of all the keys (in
another dict, perhaps), or otherwise coming up with some scheme to detect when
no longer-match is possible (because the current query string is not a prefix
of any key).  This scheme would have to be able to be quickly updated on
inserts and deletes of keys.</p>
<dl class="class">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner">
<em class="property">class </em><tt class="descclassname">wff_language.regex_trie_dict_scanner.</tt><tt class="descname">RegexTrieDictScanner</tt><big>(</big><em>regexTrieDict</em><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>This class uses the keys of a RegexTrieDict as tokens.</p>
<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.assertEndOfSeq">
<tt class="descname">assertEndOfSeq</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.assertEndOfSeq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.assertEndOfSeq" title="Permalink to this definition">¶</a></dt>
<dd><p>Asserts that there are no more elements in the current sequence.
Will empty out the buffer of elements and of possible matches.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.clear">
<tt class="descname">clear</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the tokenizer to its initial condition.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.clearTokenDataDeque">
<tt class="descname">clearTokenDataDeque</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.clearTokenDataDeque"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.clearTokenDataDeque" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current deque of matches empty.  This may be useful in an
algorithm, or to free memory in a long sequence.  Does not alter anything
else, including the current query and any saved possible-match value.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.currentSeqIsValid">
<tt class="descname">currentSeqIsValid</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.currentSeqIsValid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.currentSeqIsValid" title="Permalink to this definition">¶</a></dt>
<dd><p>Return True if the current sequence being tokenized is still valid.
Return False otherwise.  A sequence becomes invalid if there are any
inserts or deletes in the underlying Trie.  This is just for informational
purposes, since any attempt to insert an element in an invalid sequence
will automatically call resetSeqAfterFlushing first and reset the sequence.</p>
</dd></dl>

<dl class="attribute">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.digits">
<tt class="descname">digits</tt><em class="property"> = set(['1', '0', '3', '2', '5', '4', '7', '6', '9', '8'])</em><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.digits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.getTokenDataDeque">
<tt class="descname">getTokenDataDeque</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.getTokenDataDeque"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.getTokenDataDeque" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the deque of matches generated by insertSeqElem calls.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.insertSeqElem">
<tt class="descname">insertSeqElem</tt><big>(</big><em>char</em>, <em>miscData=None</em><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.insertSeqElem"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.insertSeqElem" title="Permalink to this definition">¶</a></dt>
<dd><p>Insert the next element from the sequence being tokenized.  If
inserting the element results in a match (including detecting an
unrecognizable token) the matching token (or sequence of elements) is
appended to the result deque, which the getTokenDataDeque method returns.
This function returns True until some string has been recognized as not
being stored in the tree, after which it returns False (this signals
that some higher-level error-handling needs to be done).</p>
<p>A future modification might allow multiple query instances,
essentially a wrapper-class for pointers to nodes in the tree.</p>
<p>The format of the tokenDataDeque is a deque of TokenData class instances.</p>
<p>In lexical analysis:</p>
<p>The optional miscData argument is miscellaneous data which is associated
with the query chars (in particular, line numbers can be stored and
&#8220;passed up&#8221; for better error reporting).</p>
<p>Inserting the empty string &#8220;&#8221; is equivalent to asserting the end of the
query string (and is how assertEndOfSeq() is implemented).</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.printTokenDeque">
<tt class="descname">printTokenDeque</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.printTokenDeque"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.printTokenDeque" title="Permalink to this definition">¶</a></dt>
<dd><p>Debugging routine, print out all the strings in tokenDataDeque.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.resetSeq">
<tt class="descname">resetSeq</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.resetSeq"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.resetSeq" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the sequence from previous insertSeqElem calls, i.e.,
start the next insertion back at the root node.  All of the saved possible
token matches are deleted and not reported: this is a cold reset.</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.resetSeqAfterFlushing">
<tt class="descname">resetSeqAfterFlushing</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.resetSeqAfterFlushing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.resetSeqAfterFlushing" title="Permalink to this definition">¶</a></dt>
<dd><p>This flushes out the buffer of possible saved token matches before
resetting the sequence of elements (back to start at the root).</p>
</dd></dl>

<dl class="method">
<dt id="wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.setMatchLongest">
<tt class="descname">setMatchLongest</tt><big>(</big><em>boolVal</em><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#RegexTrieDictScanner.setMatchLongest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.RegexTrieDictScanner.setMatchLongest" title="Permalink to this definition">¶</a></dt>
<dd><p>Set True if longest matches should be found in insertSeqElem queries,
False if shortest.  The default in initialization and after a clear()
is True.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="wff_language.regex_trie_dict_scanner.TokenData">
<em class="property">class </em><tt class="descclassname">wff_language.regex_trie_dict_scanner.</tt><tt class="descname">TokenData</tt><big>(</big><em>validToken</em>, <em>tokenString</em>, <em>data</em>, <em>elemData</em><big>)</big><a class="reference internal" href="_modules/wff_language/regex_trie_dict_scanner.html#TokenData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#wff_language.regex_trie_dict_scanner.TokenData" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>This class is the basic container used by the RegexTrieDictScanner.  It
holds a token (which may be an invalid token) and also some related data.
The tokenDataDeque of the RegexTrieDictScanner (such as from getTokenDataDeque()
calls) contains TokenData objects.</p>
<p>A convention which is used in the language parsing application is that the
self.data field contains a tuple of information where the first, 0th item in
the tuple is a string giving the sort or type of language element, and the
second or later elements contain data dependent of the sort of element.  The
current module does not set any of these values, however, and so does have
any need to know about or apply that convention.</p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">2.1.14. wff_language.regex_trie_dict_scanner module</a><ul>
<li><a class="reference internal" href="#regextriedictscanner">2.1.14.1. RegexTrieDictScanner</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="wff_language.regex_trie_dict_lexer.html"
                        title="previous chapter">2.1.13. wff_language.regex_trie_dict_lexer module</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="wff_language.set_package_attribute.html"
                        title="next chapter">2.1.15. wff_language.set_package_attribute module</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/wff_language.regex_trie_dict_scanner.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="wff_language.set_package_attribute.html" title="2.1.15. wff_language.set_package_attribute module"
             >next</a> |</li>
        <li class="right" >
          <a href="wff_language.regex_trie_dict_lexer.html" title="2.1.13. wff_language.regex_trie_dict_lexer module"
             >previous</a> |</li>
        <li><a href="index.html">Skolem  documentation</a> &raquo;</li>
          <li><a href="wff_language.html" >2. wff_language package</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, Allen Barker.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>